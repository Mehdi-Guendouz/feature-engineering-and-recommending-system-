{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1187,"sourceType":"datasetVersion","datasetId":626}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-23T16:52:18.174783Z","iopub.execute_input":"2023-12-23T16:52:18.175139Z","iopub.status.idle":"2023-12-23T16:52:18.479493Z","shell.execute_reply.started":"2023-12-23T16:52:18.175113Z","shell.execute_reply":"2023-12-23T16:52:18.478334Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/movielens-100k-dataset/ml-100k/u.occupation\n/kaggle/input/movielens-100k-dataset/ml-100k/u1.base\n/kaggle/input/movielens-100k-dataset/ml-100k/u.info\n/kaggle/input/movielens-100k-dataset/ml-100k/u4.test\n/kaggle/input/movielens-100k-dataset/ml-100k/u.item\n/kaggle/input/movielens-100k-dataset/ml-100k/README\n/kaggle/input/movielens-100k-dataset/ml-100k/u1.test\n/kaggle/input/movielens-100k-dataset/ml-100k/ua.test\n/kaggle/input/movielens-100k-dataset/ml-100k/u.data\n/kaggle/input/movielens-100k-dataset/ml-100k/u5.test\n/kaggle/input/movielens-100k-dataset/ml-100k/mku.sh\n/kaggle/input/movielens-100k-dataset/ml-100k/u5.base\n/kaggle/input/movielens-100k-dataset/ml-100k/u.user\n/kaggle/input/movielens-100k-dataset/ml-100k/ub.base\n/kaggle/input/movielens-100k-dataset/ml-100k/u4.base\n/kaggle/input/movielens-100k-dataset/ml-100k/u2.test\n/kaggle/input/movielens-100k-dataset/ml-100k/ua.base\n/kaggle/input/movielens-100k-dataset/ml-100k/u3.test\n/kaggle/input/movielens-100k-dataset/ml-100k/u.genre\n/kaggle/input/movielens-100k-dataset/ml-100k/allbut.pl\n/kaggle/input/movielens-100k-dataset/ml-100k/u3.base\n/kaggle/input/movielens-100k-dataset/ml-100k/u2.base\n/kaggle/input/movielens-100k-dataset/ml-100k/ub.test\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Specify the file path to your MovieLens 100k data\nfile_path = '/kaggle/input/movielens-100k-dataset/ml-100k/u.data'\n\n# Load the data into a pandas DataFrame\ncolumns = ['User_id', 'Movie_id', 'Rating', 'Timestamp']\ndf = pd.read_csv(file_path, names=columns, delimiter='\\t')  # Use delimiter=',' for CSV files\n\n# Omit the Timestamp column\ndf = df[['User_id', 'Movie_id', 'Rating']]\n\n# Display the first few rows of the DataFrame\nprint(df.head())\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:52:22.882125Z","iopub.execute_input":"2023-12-23T16:52:22.882576Z","iopub.status.idle":"2023-12-23T16:52:22.966929Z","shell.execute_reply.started":"2023-12-23T16:52:22.882548Z","shell.execute_reply":"2023-12-23T16:52:22.965694Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"   User_id  Movie_id  Rating\n0      196       242       3\n1      186       302       3\n2       22       377       1\n3      244        51       2\n4      166       346       1\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX = df[['User_id', 'Movie_id']]\ny = df['Rating']\n\n# Split the data into train and test sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Display the shapes of the resulting train and test sets\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of y_test:\", y_test.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:52:25.637430Z","iopub.execute_input":"2023-12-23T16:52:25.638195Z","iopub.status.idle":"2023-12-23T16:52:26.185310Z","shell.execute_reply.started":"2023-12-23T16:52:25.638162Z","shell.execute_reply":"2023-12-23T16:52:26.184412Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Shape of X_train: (80000, 2)\nShape of X_test: (20000, 2)\nShape of y_train: (80000,)\nShape of y_test: (20000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming you have the train and test sets (X_train, X_test, y_train, y_test)\n\n# Create dense training data table\ntraining_data = pd.DataFrame(index=X_train['User_id'].unique(), columns=X_train['Movie_id'].unique())\ntraining_data = training_data.sort_index(axis=0).sort_index(axis=1)  # Sort indices for consistency\n\n# Fill in the ratings from the training set\nfor i, row in X_train.iterrows():\n    user_id, movie_id, rating = row['User_id'], row['Movie_id'], y_train[i]\n    training_data.at[user_id, movie_id] = rating\n\n\n# Create dense testing data table\ntesting_data = pd.DataFrame(index=X_test['User_id'].unique(), columns=X_test['Movie_id'].unique())\ntesting_data = testing_data.sort_index(axis=0).sort_index(axis=1)  # Sort indices for consistency\n\n# Fill in the ratings from the testing set\nfor i, row in X_test.iterrows():\n    user_id, movie_id, rating = row['User_id'], row['Movie_id'], y_test[i]\n    testing_data.at[user_id, movie_id] = rating\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:52:34.838801Z","iopub.execute_input":"2023-12-23T16:52:34.839555Z","iopub.status.idle":"2023-12-23T16:52:42.005101Z","shell.execute_reply.started":"2023-12-23T16:52:34.839513Z","shell.execute_reply":"2023-12-23T16:52:42.003566Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Fill missing values with 0 or another appropriate value\ntraining_data = training_data.fillna(0)\n\n\n# Display the dense training data table\nprint(\"Dense Training Data Table:\")\nprint(training_data.head())\n\n# Fill missing values with 0 or another appropriate value\ntesting_data = testing_data.fillna(0)\n\n# Display the dense testing data table\nprint(\"\\nDense Testing Data Table:\")\nprint(testing_data.head())","metadata":{"execution":{"iopub.status.busy":"2023-12-23T16:53:53.255349Z","iopub.execute_input":"2023-12-23T16:53:53.255692Z","iopub.status.idle":"2023-12-23T16:53:53.993561Z","shell.execute_reply.started":"2023-12-23T16:53:53.255666Z","shell.execute_reply":"2023-12-23T16:53:53.992949Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Dense Training Data Table:\n   1     2     3     4     5     6     7     8     9     10    ...  1668  \\\n1     0     3     4     0     3     0     4     0     5     3  ...     0   \n2     4     0     0     0     0     0     0     0     0     0  ...     0   \n3     0     0     0     0     0     0     0     0     0     0  ...     0   \n4     0     0     0     0     0     0     0     0     0     0  ...     0   \n5     4     0     0     0     0     0     0     0     0     0  ...     0   \n\n   1670  1671  1672  1673  1676  1678  1679  1680  1681  \n1     0     0     0     0     0     0     0     0     0  \n2     0     0     0     0     0     0     0     0     0  \n3     0     0     0     0     0     0     0     0     0  \n4     0     0     0     0     0     0     0     0     0  \n5     0     0     0     0     0     0     0     0     0  \n\n[5 rows x 1653 columns]\n\nDense Testing Data Table:\n   1     2     3     4     5     6     7     8     9     10    ...  1648  \\\n1     5     0     0     3     0     5     0     1     0     0  ...     0   \n2     0     0     0     0     0     0     0     0     0     2  ...     0   \n3     0     0     0     0     0     0     0     0     0     0  ...     0   \n4     0     0     0     0     0     0     0     0     0     0  ...     0   \n5     0     3     0     0     0     0     0     0     0     0  ...     0   \n\n   1649  1655  1656  1658  1669  1674  1675  1677  1682  \n1     0     0     0     0     0     0     0     0     0  \n2     0     0     0     0     0     0     0     0     0  \n3     0     0     0     0     0     0     0     0     0  \n4     0     0     0     0     0     0     0     0     0  \n5     0     0     0     0     0     0     0     0     0  \n\n[5 rows x 1411 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming you have the dense training data table (training_data)\n\n# Calculate the global mean\nglobal_mean = training_data.values[training_data.values != 0].mean()\n\nprint(\"Global Mean of Ratings in the Training Data:\", global_mean)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T17:00:38.221275Z","iopub.execute_input":"2023-12-23T17:00:38.221590Z","iopub.status.idle":"2023-12-23T17:00:38.246868Z","shell.execute_reply.started":"2023-12-23T17:00:38.221565Z","shell.execute_reply":"2023-12-23T17:00:38.245702Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Global Mean of Ratings in the Training Data: 3.5312625\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate items' bias (bi) for each item i\nitems_bias = {}\n# movie_columns = training_data.columns\n# print(movie_columns.head())\nfor item_id in movie_columns:\n    item_ratings = training_data[item_id][training_data[item_id] != 0]  # Select non-zero ratings\n    num_ratings = len(item_ratings)\n    if num_ratings > 0:\n        bi = (item_ratings - global_mean).sum() / num_ratings\n        items_bias[item_id] = bi\n    else:\n        items_bias[item_id] = 0  # Handle the case where the item has no ratings\n\n# Create a new column 'itemBias' in the training data\ntraining_data['itemBias'] = items_bias\n\n# Display the training data with the 'itemBias' column\nprint(\"Training Data with Item Bias:\")\nprint(training_data.head())\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T17:09:52.335123Z","iopub.execute_input":"2023-12-23T17:09:52.335448Z","iopub.status.idle":"2023-12-23T17:09:52.932639Z","shell.execute_reply.started":"2023-12-23T17:09:52.335422Z","shell.execute_reply":"2023-12-23T17:09:52.931654Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Training Data with Item Bias:\n   1  2  3  4  5  6  7  8  9  10  ...  1670  1671  1672  1673  1676  1678  \\\n1  0  3  4  0  3  0  4  0  5   3  ...     0     0     0     0     0     0   \n2  4  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n3  0  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n4  0  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n5  4  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n\n   1679  1680  1681  itemBias  \n1     0     0     0  0.334067  \n2     0     0     0 -0.327379  \n3     0     0     0 -0.429813  \n4     0     0     0  0.017870  \n5     0     0     0 -0.284886  \n\n[5 rows x 1654 columns]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_41/563120244.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  training_data['itemBias'] = items_bias\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming you have the dense training data table (training_data), global_mean, and users_bias calculated earlier\n\n# Calculate items' bias (bi) for each item i\nitems_bias = {}\n# traning_data_exclude = training_data.columns[:-2]\nprint(training_data.head())\n# for item_id in training_data.columns[:-2]:  # Exclude the 'userBias' and 'itemBias' columns\n#     item_ratings = training_data[item_id][training_data[item_id] != 0]  # Select non-zero ratings\n#     num_ratings = len(item_ratings)\n#     if num_ratings > 0:\n#         bi = (item_ratings - (global_mean + training_data['userBias'])).sum() / num_ratings\n#         items_bias[item_id] = bi\n#     else:\n#         items_bias[item_id] = 0  # Handle the case where the item has no ratings\n\n# # Create a new column 'itemBias' in the training data\n# training_data['itemBias'] = training_data.columns[:-2].map(items_bias.get)\n\n# # Display the training data with the 'itemBias' column\n# print(\"Training Data with Item Bias:\")\n# print(training_data.head())\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T11:41:14.084412Z","iopub.execute_input":"2023-12-23T11:41:14.084908Z","iopub.status.idle":"2023-12-23T11:41:14.113207Z","shell.execute_reply.started":"2023-12-23T11:41:14.084871Z","shell.execute_reply":"2023-12-23T11:41:14.112053Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"   1  2  3  4  5  6  7  8  9  10  ...  1670  1671  1672  1673  1676  1678  \\\n1  0  3  4  0  3  0  4  0  5   3  ...     0     0     0     0     0     0   \n2  4  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n3  0  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n4  0  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n5  4  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n\n   1679  1680  1681  userBias  \n1     0     0     0  0.156237  \n2     0     0     0  0.273085  \n3     0     0     0 -0.731263  \n4     0     0     0  0.968737  \n5     0     0     0 -0.668944  \n\n[5 rows x 1654 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate users' bias (bu) for each user u\nusers_bias = {}\nfor user_id, row in training_data.iterrows():\n    user_ratings = row[:-1]  # Exclude 'itemBias' column\n    rated_items = user_ratings[user_ratings != 0]\n    \n    # Calculate the number of items rated by user u and adjust for a small constant\n    num_rated_items = 0.99 + abs(len(rated_items))\n    \n    if num_rated_items > 0:\n        bu = (rated_items - (global_mean + training_data['itemBias'])).sum() / num_rated_items\n        users_bias[user_id] = bu\n    else:\n        users_bias[user_id] = 0  # Handle the case where the user has no ratings\n\n# Create a new column 'userBias' in the training data\ntraining_data['userBias'] = training_data.index.map(users_bias)\n\n# Display the training data with the 'userBias' column\nprint(\"Training Data with User Bias:\")\nprint(training_data.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T17:11:03.920090Z","iopub.execute_input":"2023-12-23T17:11:03.920442Z","iopub.status.idle":"2023-12-23T17:11:05.224403Z","shell.execute_reply.started":"2023-12-23T17:11:03.920416Z","shell.execute_reply":"2023-12-23T17:11:05.222928Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Training Data with User Bias:\n   1  2  3  4  5  6  7  8  9  10  ...  1671  1672  1673  1676  1678  1679  \\\n1  0  3  4  0  3  0  4  0  5   3  ...     0     0     0     0     0     0   \n2  4  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n3  0  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n4  0  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n5  4  0  0  0  0  0  0  0  0   0  ...     0     0     0     0     0     0   \n\n   1680  1681  itemBias  userBias  \n1     0     0  0.334067  0.134723  \n2     0     0 -0.327379  0.199249  \n3     0     0 -0.429813 -0.493094  \n4     0     0  0.017870  1.044080  \n5     0     0 -0.284886 -0.353129  \n\n[5 rows x 1655 columns]\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_41/3943886886.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  training_data['userBias'] = training_data.index.map(users_bias)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a copy of the training data to store the filled values\ntraining_data_filled = training_data.copy()\n\n# Iterate through each row\nfor user_id, row in training_data.iterrows():\n    for movie_id in training_data.columns[:-3]:\n        # Check if the value is missing (original value is 0)\n        if row[movie_id] == 0:\n            # Update the missing value using the formula: missing(u, i) = bu + bi + global_mean\n            filled_value = users_bias[user_id] + items_bias[movie_id] + global_mean\n            training_data_filled.at[user_id, movie_id] = filled_value\n\n# Display the filled training data\nprint(\"Filled Training Data:\")\nprint(training_data_filled.head())","metadata":{"execution":{"iopub.status.busy":"2023-12-23T17:22:13.171367Z","iopub.execute_input":"2023-12-23T17:22:13.171720Z","iopub.status.idle":"2023-12-23T17:22:44.115960Z","shell.execute_reply.started":"2023-12-23T17:22:13.171691Z","shell.execute_reply":"2023-12-23T17:22:44.114995Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Filled Training Data:\n          1         2         3         4         5         6         7  \\\n1  4.000053  3.000000  4.000000  3.683856  3.000000  3.781782  4.000000   \n2  4.000000  3.403133  3.300699  3.748382  3.445626  3.846308  4.011162   \n3  3.372236  2.710790  2.608355  3.056039  2.753283  3.153965  3.318818   \n4  4.909410  4.247964  4.145529  4.593213  4.290457  4.691139  4.855992   \n5  4.000000  2.850755  2.748320  3.196004  2.893248  3.293930  3.458783   \n\n          8         9        10  ...      1671      1672      1673      1676  \\\n1  4.070770  5.000000  3.000000  ...  1.134723  2.134723  3.134723  2.134723   \n2  4.135296  4.107199  4.091141  ...  1.199249  2.199249  3.199249  2.199249   \n3  3.442953  3.414856  3.398798  ...  0.506906  1.506906  2.506906  1.506906   \n4  4.980127  4.952030  4.935972  ...  2.044080  3.044080  4.044080  3.044080   \n5  3.582918  3.554821  3.538763  ...  0.646871  1.646871  2.646871  1.646871   \n\n       1678      1679      1680  1681  itemBias  userBias  \n1  1.134723  3.134723  2.134723     0  0.334067  0.134723  \n2  1.199249  3.199249  2.199249     0 -0.327379  0.199249  \n3  0.506906  2.506906  1.506906     0 -0.429813 -0.493094  \n4  2.044080  4.044080  3.044080     0  0.017870  1.044080  \n5  0.646871  2.646871  1.646871     0 -0.284886 -0.353129  \n\n[5 rows x 1655 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\n# Fill NaN values with a specific value (e.g., 0)\ntraining_data_filled = training_data_filled.fillna(0)\n\n# Specify the number of components (you can adjust this based on your requirements)\nn_components = min(training_data_filled.shape) - 1\n\n# Create TruncatedSVD instance and fit_transform the filled training data\nsvd = TruncatedSVD(n_components=n_components)\nU = svd.fit_transform(training_data_filled)\nS = np.diag(svd.singular_values_)\nV = svd.components_\n\n# Display the shapes of U, S, and V\nprint(\"Shape of U:\", U.shape)\nprint(\"Shape of S:\", S.shape)\nprint(\"Shape of V:\", V.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T17:32:47.832608Z","iopub.execute_input":"2023-12-23T17:32:47.832968Z","iopub.status.idle":"2023-12-23T17:32:50.099452Z","shell.execute_reply.started":"2023-12-23T17:32:47.832938Z","shell.execute_reply":"2023-12-23T17:32:50.098786Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Shape of U: (943, 942)\nShape of S: (942, 942)\nShape of V: (942, 1655)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Specify the number of components (you can adjust this based on your requirements)\nn_components = 10  # Replace 10 with the desired number of components\n\n# Reduce U, S, and V\nU_approx = U[:, :n_components]\nS_approx = S[:n_components, :n_components]\nV_approx = V[:n_components, :]\n\n# Display the shapes of the reduced matrices\nprint(\"Shape of U_approx:\", U_approx.shape)\nprint(\"Shape of S_approx:\", S_approx.shape)\nprint(\"Shape of V_approx:\", V_approx.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T17:33:27.108956Z","iopub.execute_input":"2023-12-23T17:33:27.109371Z","iopub.status.idle":"2023-12-23T17:33:27.116264Z","shell.execute_reply.started":"2023-12-23T17:33:27.109345Z","shell.execute_reply":"2023-12-23T17:33:27.114911Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Shape of U_approx: (943, 10)\nShape of S_approx: (10, 10)\nShape of V_approx: (10, 1655)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate Z\nZ_approx = np.dot(U_approx, np.dot(S_approx, V_approx))\n\n# Display the shape of Z\nprint(\"Shape of Z_approx:\", Z_approx.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-23T17:34:26.113173Z","iopub.execute_input":"2023-12-23T17:34:26.113535Z","iopub.status.idle":"2023-12-23T17:34:26.124171Z","shell.execute_reply.started":"2023-12-23T17:34:26.113509Z","shell.execute_reply":"2023-12-23T17:34:26.123354Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Shape of Z_approx: (943, 1655)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming testing_data and Z_approx are NumPy arrays\nnon_zero_mask = testing_data != 0\nmae_masked = np.mean(np.abs(testing_data[non_zero_mask] - Z_approx[non_zero_mask]))\nprint(\"Mean Absolute Error (MAE) for non-zero values:\", mae_masked)","metadata":{},"execution_count":null,"outputs":[]}]}